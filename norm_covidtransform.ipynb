{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4fe617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24f51381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some convenience helper functions used throughout the notebook\n",
    "\n",
    "\n",
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5fa095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):  #the final linear layer after decoding\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "\n",
    "    def __init__(self, d_model): \n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, 1)\n",
    "        #note that in our case, we want our output to be the number of predicted deaths, so there is no vocab size\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = self.proj(x) #we will get a float out which is okay\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3f357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) \n",
    "#Note we need deepcopies because the layers must be different, otherwise they will be repeated layers via reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73da66ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See https://arxiv.org/pdf/1607.06450.pdf for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout) #dropout to prevent overfitting\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x))) #returns x+sublayer(x) incl norm and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdbc166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined later)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2) #one for after self-att, one for after ff, see below\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) \n",
    "        #Note that the 2nd input needs to be a function (representing the sublayer)\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0626ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask): #memory refers to context from encoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x) \n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3) #one for each of the above\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c52aba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size): #also known as causal mask\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    ) #torch.triu returns the uppertriangular matrix of input matrix\n",
    "    return subsequent_mask == 0 #this inverts the 1s and 0s, giving upper triangular of 0s, lower triangular of 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "debc34c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None): #to be used later for selt and cross att sublayers\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1) #note that this d_k is typically the head size\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        #print(scores.size(),mask.size())\n",
    "        scores = scores.masked_fill(mask == 0, -1e9) #fills upper triangular with -1e9\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn #returning attention matrix is good for data visualisation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d82749cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1): #h is the number of heads\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) #.view() changes dimension without changing original \n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k) #unstacks\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a9e0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu())) #standard ff relu layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e6a512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: MAJOR changes from here from on\n",
    "\"\"\"\n",
    "We need to embed 3 parameters: Variables, Country/Location, Time.\n",
    "Time embeddding will be done via the standard sinusoidal positional encoding in d dim (FUTURE WORK: use learnt time embeddings as suggested by https://arxiv.org/abs/2109.12218)\n",
    "Country/Loc & Variable Embedding will be learnt. Var emb will be d-1 dim, Country/Loc will be d-1 dim and concatenated to the associated value. \n",
    "Note: In the source above they had a learnt time emb, and concatenated time embedding to the value, then added the spatial embedding. In ours we will concatenate var,loc emb and add positional emb.\n",
    "\"\"\"\n",
    "class Embeddings(nn.Module): \n",
    "    def __init__(self, d_emb, vocab): #d_emb will be d_model-1, vocab will be num of countries/num of vars\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_emb)\n",
    "        self.d_emb = d_emb\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_emb)\n",
    "    \n",
    "\"\"\"\n",
    "Note that we have 3 variables of 14 past days, so we need to append appropriately. Also, the format of our input is\n",
    "[<14 days of past cases><past vacc><13 days of past deaths><day 14><day 14 repeated for decoder><day 15 and day 16>]\n",
    "our target outcome is [<day 15,16,17>]\n",
    "\"\"\"\n",
    "\n",
    "class InpPositionalEncoding(nn.Module):\n",
    "    \"Implement the Input PE function. \"\n",
    "\n",
    "    def __init__(self, d_model, dropout, past, var_num): #past is no. of days of past data\n",
    "        super(InpPositionalEncoding, self).__init__()\n",
    "        self.past = past\n",
    "        self.var_num = var_num\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(past, d_model)\n",
    "        position = torch.arange(0, past).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) \n",
    "        self.register_buffer(\"pe\", pe) #buffers are by default persistent\n",
    "        \n",
    "\n",
    "    def forward(self, x): \n",
    "        past = self.past\n",
    "        var_num = self.var_num\n",
    "        assert x.size(1)%past==0 #x should have shape batchsize*(past*var_num)*d\n",
    "        \n",
    "        for k in range(var_num):\n",
    "            x[:, k*past:(k+1)*past] = x[:, k*past:(k+1)*past] + self.pe[:, :past].requires_grad_(False) \n",
    "        return self.dropout(x)\n",
    "\n",
    "class OutPositionalEncoding(nn.Module):\n",
    "    \"Implement the Output PE function. \"\n",
    "\n",
    "    def __init__(self, d_model, dropout, past, pred): #pred is no. of days want to pred\n",
    "        super(OutPositionalEncoding, self).__init__()\n",
    "        self.past = past\n",
    "        self.pred = pred\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(past+pred-1, d_model) #Note the -1 is due to the repeated day 14\n",
    "        position = torch.arange(0, past+pred-1).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe) \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #x has shape batchsize*(size<=pred)*d_model\n",
    "        pred = self.pred\n",
    "        if x.size(1)==pred:\n",
    "            x+=self.pe[:, -pred:].requires_grad_(False) \n",
    "        elif x.size(1)<pred:\n",
    "            x += self.pe[:, -pred:-pred+x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9445bb0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def example_positional1():\\n    pe = InpPositionalEncoding(20, 0, 14, 3)\\n    y = pe.forward(torch.zeros(1, 42, 20)) #first index is batches, 2nd index is dates, 3rd index is dim of model\\n    print([y[0, :, dim] for dim in [3,4,5]])\\n    data = pd.concat(\\n        [\\n            pd.DataFrame(\\n                {\\n                    \"embedding\": y[0, :, dim],\\n                    \"dimension\": dim,\\n                    \"position\": list(range(42)),\\n                }\\n            )\\n            for dim in [4,5,6,7]\\n        ]\\n    )\\n\\n    return (\\n        alt.Chart(data)\\n        .mark_line()\\n        .properties(width=800)\\n        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\\n        .interactive()\\n    )\\n\\ndef example_positional2():\\n    pe = OutPositionalEncoding(20, 0, 200, 100)\\n    y = pe.forward(torch.zeros(1, 86, 20)) #first index is batches, 2nd index is dates, 3rd index is dim of model\\n    print([y[0, :, dim] for dim in [3,4,5]])\\n    data = pd.concat(\\n        [\\n            pd.DataFrame(\\n                {\\n                    \"embedding\": y[0, :, dim],\\n                    \"dimension\": dim,\\n                    \"position\": list(range(86)),\\n                }\\n            )\\n            for dim in [4,5,6,7]\\n        ]\\n    )\\n\\n    return (\\n        alt.Chart(data)\\n        .mark_line()\\n        .properties(width=800)\\n        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\\n        .interactive()\\n    )\\n\\nshow_example(example_positional1)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def example_positional1():\n",
    "    pe = InpPositionalEncoding(20, 0, 14, 3)\n",
    "    y = pe.forward(torch.zeros(1, 42, 20)) #first index is batches, 2nd index is dates, 3rd index is dim of model\n",
    "    print([y[0, :, dim] for dim in [3,4,5]])\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"embedding\": y[0, :, dim],\n",
    "                    \"dimension\": dim,\n",
    "                    \"position\": list(range(42)),\n",
    "                }\n",
    "            )\n",
    "            for dim in [4,5,6,7]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(data)\n",
    "        .mark_line()\n",
    "        .properties(width=800)\n",
    "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "def example_positional2():\n",
    "    pe = OutPositionalEncoding(20, 0, 200, 100)\n",
    "    y = pe.forward(torch.zeros(1, 86, 20)) #first index is batches, 2nd index is dates, 3rd index is dim of model\n",
    "    print([y[0, :, dim] for dim in [3,4,5]])\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"embedding\": y[0, :, dim],\n",
    "                    \"dimension\": dim,\n",
    "                    \"position\": list(range(86)),\n",
    "                }\n",
    "            )\n",
    "            for dim in [4,5,6,7]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(data)\n",
    "        .mark_line()\n",
    "        .properties(width=800)\n",
    "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "show_example(example_positional1)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25d2cd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spreademb(var_num, size):\n",
    "    x = [[k for _ in range(size)] for k in range(var_num)]\n",
    "    x = torch.LongTensor(x)\n",
    "    x = x.flatten()\n",
    "    return x\n",
    "\n",
    "spreademb(3,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81eb2b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many\n",
    "    other models.\n",
    "    \"\"\"\n",
    "#IMPORTANT: CHANGE SRC&TGT EMB?\n",
    "    def __init__(self, encoder, decoder, loc_emb, var_emb, generator, inppos, outpos, past, pred, var_num): \n",
    "        #inputs should be encoder modules\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.loc_emb = loc_emb\n",
    "        self.var_emb = var_emb\n",
    "        self.generator = generator\n",
    "        self.past = past\n",
    "        self.pred = pred\n",
    "        self.var_num = var_num\n",
    "        self.inppos = inppos\n",
    "        self.outpos = outpos\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.generator(self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask))\n",
    "    #self.encode gives the context to be passed to decoder\n",
    "\n",
    "    def encode(self, src, src_mask): \n",
    "        \"src will be batchsize*[<country><case num for past days><vacc num><death num>]\"\n",
    "        \"Create emb to be concat\"\n",
    "        loc = src[:, 0].type(torch.LongTensor).to(device=0) #shape batchsize*(d-1) ; convert to long for emb layer\n",
    "        loc_emb = self.loc_emb(loc) \n",
    "        loc_emb = loc_emb.view(src.size(0),1,loc_emb.size(1)) #shape batchsize*1*(d-1)\n",
    "        #print(src.size(0),src.size(1)-1,loc_emb.size(2))\n",
    "        \n",
    "        embtable = torch.zeros(src.size(0),src.size(1)-1,loc_emb.size(2)).to(device=0)\n",
    "        #shape batchsize*(past*var_num = 3*14)*(d-1)\n",
    "\n",
    "        embtable += loc_emb #adds loc emb\n",
    "        embtable += self.var_emb(spreademb(self.var_num, self.past).to(device=0)) #adds var emb for all 3 vars spread out\n",
    "        \"Concat emb\"\n",
    "        src = src[:, 1:]\n",
    "        src = src.view(src.size(0),src.size(1),1) #add another dim to cat later\n",
    "        src = torch.cat((src,embtable),dim=2)\n",
    "\n",
    "        return self.encoder(self.inppos(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        \"tgt will be batchsize*[<country><death num for (x < pred) days>]\"\n",
    "        loc = tgt[:, 0].type(torch.LongTensor).to(device=0) #shape batchsize*(d-1)\n",
    "        loc_emb = self.loc_emb(loc) \n",
    "        \n",
    "        loc_emb = loc_emb.view(tgt.size(0),1,loc_emb.size(1)) #shape batchsize*1*(d-1)\n",
    "        embtable = torch.zeros((tgt.size(0),tgt.size(1)-1,loc_emb.size(2))).to(device=0)\n",
    "        #shape batchsize*(pred)*(d-1)\n",
    "        embtable += loc_emb #adds loc emb\n",
    "        embtable += self.var_emb(torch.LongTensor([self.var_num-1 for _ in range(tgt.size(1)-1)]).to(device=0)) #var emb of deaths\n",
    "        \n",
    "        tgt = tgt[:, 1:] #shape batchsize*(pred)\n",
    "        tgt = tgt.view(tgt.size(0),tgt.size(1),1) #batchsize*pred*1\n",
    "        tgt = torch.cat((tgt,embtable),dim=2) #batchsize*pred*(d)\n",
    "        \n",
    "        return self.decoder(self.outpos(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "def make_model(\n",
    "    loc_num, var_num=3, past=14, pred=3, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    inppos = InpPositionalEncoding(d_model, dropout, past, var_num)\n",
    "    outpos = OutPositionalEncoding(d_model, dropout, past, pred)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        Embeddings(d_model-1, loc_num), #loc emb \n",
    "        Embeddings(d_model-1, var_num), #var emb\n",
    "        Generator(d_model),\n",
    "        inppos,\n",
    "        outpos,\n",
    "        past, pred, var_num\n",
    "    )\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7dcb26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def inference_test():\\n    test_model = make_model(loc_num=1, var_num=2, past=5, pred=7)\\n    test_model.cuda(0)\\n    test_model.eval()\\n    src = torch.LongTensor([[0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5]]).to(device=0)\\n    src_mask = torch.ones(1, 1, 10).to(device=0)\\n\\n    memory = test_model.encode(src, src_mask)\\n    ys = torch.LongTensor([[0,5]]).to(device=0) #repeat last day\\n\\n    for i in range(6):\\n        out = test_model.decode(\\n            memory, src_mask, ys, subsequent_mask(ys.size(1)-1).type_as(src.data)\\n        )\\n        out = test_model.generator(out)\\n        out = out.view(out.size(0),out.size(1)).to(device=0)\\n        out = out[:, -1:] #take last input\\n        #print(ys.size(),out.size())\\n        ys = torch.cat(\\n            [ys, out], dim=1\\n        )\\n\\n    print(\"Example Untrained Model Prediction:\", ys)\\n\\n\\ndef run_tests():\\n    for _ in range(10):\\n        inference_test()\\n\\n\\nshow_example(run_tests)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def inference_test():\n",
    "    test_model = make_model(loc_num=1, var_num=2, past=5, pred=7)\n",
    "    test_model.cuda(0)\n",
    "    test_model.eval()\n",
    "    src = torch.LongTensor([[0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5]]).to(device=0)\n",
    "    src_mask = torch.ones(1, 1, 10).to(device=0)\n",
    "\n",
    "    memory = test_model.encode(src, src_mask)\n",
    "    ys = torch.LongTensor([[0,5]]).to(device=0) #repeat last day\n",
    "\n",
    "    for i in range(6):\n",
    "        out = test_model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)-1).type_as(src.data)\n",
    "        )\n",
    "        out = test_model.generator(out)\n",
    "        out = out.view(out.size(0),out.size(1)).to(device=0)\n",
    "        out = out[:, -1:] #take last input\n",
    "        #print(ys.size(),out.size())\n",
    "        ys = torch.cat(\n",
    "            [ys, out], dim=1\n",
    "        )\n",
    "\n",
    "    print(\"Example Untrained Model Prediction:\", ys)\n",
    "\n",
    "\n",
    "def run_tests():\n",
    "    for _ in range(10):\n",
    "        inference_test()\n",
    "\n",
    "\n",
    "show_example(run_tests)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c46d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Typically, our src will have no padding. For pred=14, pred=3,\n",
    "    The input we want for src is [<country><first 14 days of cases><.. of vacc><.. of deaths>]\n",
    "    format for tgt (when training) is [<country><day14><day15><day16><day17>]\n",
    "    \"\"\"\n",
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n",
    "\n",
    "    def __init__(self, src, tgt=None):\n",
    "    \n",
    "        self.src = src.to(device=0)\n",
    "        self.src_mask = None\n",
    "        \n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1].to(device=0) #the location will be processed by loc_emb layer and be added into dim later\n",
    "            self.raw = tgt[:, 1:-1].to(device=0) #death data (exclude country column)\n",
    "            tgt_y = tgt[:, 2:] #output of model\n",
    "            tgt_y = tgt_y.type(dtype=torch.FloatTensor) #change to float to calculate mse loss later\n",
    "            self.tgt_y = tgt_y.view(tgt_y.size(0),tgt_y.size(1),1).to(device=0)\n",
    "            self.tgt_mask = self.make_std_mask(self.raw).to(device=0)\n",
    "            #self.ntokens = (self.tgt_y != pad).data.sum()\n",
    "            self.ntokens = torch.flatten(self.tgt_y).size(0) \n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = subsequent_mask(tgt.size(-1)) #dtype uint8 \n",
    "        return tgt_mask\n",
    "\n",
    "\"\"\"function for processing raw batch data into a Batch object. rawbatch should have shape: batchsize*(3*past+pred+1)\n",
    "Format for each row of rawbatch is [<country><first 14 days of cases><.. of vacc><.. of deaths><day 15-17 of deaths>]\n",
    "rawbatch should be a torch tensor from dataloader\n",
    "\"\"\"\n",
    "def ProcessRaw(rawbatch, past=14, pred=3): \n",
    "    src = rawbatch[:, :-pred]\n",
    "    tgt = rawbatch[:, past*3:]\n",
    "    loc = rawbatch[:, 0:1]\n",
    "    tgt = torch.cat((loc,tgt),dim=1)\n",
    "    \n",
    "    return Batch(src,tgt)\n",
    "\n",
    "class TrainState:\n",
    "    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n",
    "\n",
    "    step: int = 0  # Steps in the current epoch\n",
    "    accum_step: int = 0  # Number of gradient accumulation steps\n",
    "    samples: int = 0  # total # of examples used\n",
    "    tokens: int = 0  # total # of tokens processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "411d0e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Note: We do not use label smoothing as we are not using cross entropy as our loss function. Our output is just 1 number so we also do not softmax.\"\n",
    "def run_epoch(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute, #we use nn.MSELoss()\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(),\n",
    "):\n",
    "    \"\"\"Train a single epoch\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0\n",
    "    for i, batch in enumerate(data_iter): #will yield batches (the class above)\n",
    "        out = model.forward(\n",
    "            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n",
    "        )\n",
    "        loss = loss_compute(out, batch.tgt_y) \n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss.backward()\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            train_state.tokens += batch.ntokens\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss*batch.ntokens\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.8f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                % (i, n_accum, loss, tokens / elapsed, lr)\n",
    "            )\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        del loss\n",
    "    return total_loss / total_tokens, train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ae4ce10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def example_learning_schedule():\\n    opts = [\\n        [512, 1, 4000],  # example 1\\n        [512, 1, 8000],  # example 2\\n        [256, 1, 4000],  # example 3\\n    ]\\n\\n    dummy_model = torch.nn.Linear(1, 1)\\n    learning_rates = []\\n\\n    # we have 3 examples in opts list.\\n    for idx, example in enumerate(opts):\\n        # run 20000 epoch for each example\\n        optimizer = torch.optim.Adam(\\n            dummy_model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9\\n        )\\n        lr_scheduler = LambdaLR(\\n            optimizer=optimizer, lr_lambda=lambda step: rate(step, *example)\\n        )\\n        tmp = []\\n        # take 20K dummy training steps, save the learning rate at each step\\n        for step in range(20000):\\n            tmp.append(optimizer.param_groups[0][\"lr\"])\\n            optimizer.step()\\n            lr_scheduler.step()\\n        learning_rates.append(tmp)\\n\\n    learning_rates = torch.tensor(learning_rates)\\n\\n    # Enable altair to handle more than 5000 rows\\n    alt.data_transformers.disable_max_rows()\\n\\n    opts_data = pd.concat(\\n        [\\n            pd.DataFrame(\\n                {\\n                    \"Learning Rate\": learning_rates[warmup_idx, :],\\n                    \"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\\n                        warmup_idx\\n                    ],\\n                    \"step\": range(20000),\\n                }\\n            )\\n            for warmup_idx in [0, 1, 2]\\n        ]\\n    )\\n\\n    return (\\n        alt.Chart(opts_data)\\n        .mark_line()\\n        .properties(width=600)\\n        .encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\\n        .interactive()\\n    )\\n\\n\\nexample_learning_schedule()'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    Returns learning rate according to Adam optimizer\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    ) \n",
    "\n",
    "'''def example_learning_schedule():\n",
    "    opts = [\n",
    "        [512, 1, 4000],  # example 1\n",
    "        [512, 1, 8000],  # example 2\n",
    "        [256, 1, 4000],  # example 3\n",
    "    ]\n",
    "\n",
    "    dummy_model = torch.nn.Linear(1, 1)\n",
    "    learning_rates = []\n",
    "\n",
    "    # we have 3 examples in opts list.\n",
    "    for idx, example in enumerate(opts):\n",
    "        # run 20000 epoch for each example\n",
    "        optimizer = torch.optim.Adam(\n",
    "            dummy_model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9\n",
    "        )\n",
    "        lr_scheduler = LambdaLR(\n",
    "            optimizer=optimizer, lr_lambda=lambda step: rate(step, *example)\n",
    "        )\n",
    "        tmp = []\n",
    "        # take 20K dummy training steps, save the learning rate at each step\n",
    "        for step in range(20000):\n",
    "            tmp.append(optimizer.param_groups[0][\"lr\"])\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "        learning_rates.append(tmp)\n",
    "\n",
    "    learning_rates = torch.tensor(learning_rates)\n",
    "\n",
    "    # Enable altair to handle more than 5000 rows\n",
    "    alt.data_transformers.disable_max_rows()\n",
    "\n",
    "    opts_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Learning Rate\": learning_rates[warmup_idx, :],\n",
    "                    \"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\n",
    "                        warmup_idx\n",
    "                    ],\n",
    "                    \"step\": range(20000),\n",
    "                }\n",
    "            )\n",
    "            for warmup_idx in [0, 1, 2]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(opts_data)\n",
    "        .mark_line()\n",
    "        .properties(width=600)\n",
    "        .encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "example_learning_schedule()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c31202ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch_size, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.randint(1, V, size=(batch_size, 10))\n",
    "        data[:, 0] = 1\n",
    "        src = data.requires_grad_(False).clone().detach()\n",
    "        tgt = data.requires_grad_(False).clone().detach()\n",
    "        yield Batch(src, tgt, 0)\n",
    "\n",
    "def inference(model, src, src_mask, pred): #input trained model\n",
    "    memory = model.encode(src, src_mask)\n",
    "    loc = src[:, 0:]\n",
    "    initial = src[:, -1:] #repeat last day\n",
    "    ys = torch.cat([loc,initial], dim=1)\n",
    "\n",
    "    for i in range(pred):\n",
    "        out = test_model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)-1).type_as(src.data)\n",
    "        )\n",
    "        out = test_model.generator(out)\n",
    "        out = torch.round(out).type(torch.LongTensor) #we round off here so we get int death predictions\n",
    "        out = out.view(out.size(0),out.size(1))\n",
    "        out = out[:, -1:] #take last input\n",
    "        #print(ys.size(),out.size())\n",
    "        ys = torch.cat(\n",
    "            [ys, out], dim=1\n",
    "        )\n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "010aa4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"We do not need a collate batch function as we have preprocessed all our data including shuffle\"\n",
    "class DfParser(torch.utils.data.Dataset): #for torch dataloader later\n",
    " \n",
    "  def __init__(self,file_name):\n",
    "    df=pd.read_csv(file_name)\n",
    "    x=df.iloc[:,1:].values\n",
    " \n",
    "    self.x=torch.tensor(x).type(torch.FloatTensor)\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.x)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x[idx]\n",
    "\n",
    "def create_dataloaders(\n",
    "    device,\n",
    "    batch_size=12000,\n",
    "    is_distributed=True,\n",
    "):\n",
    "    # def create_dataloaders(batch_size=12000):\n",
    "    train, valid, test = DfParser('norm_train.csv'), DfParser('norm_valid.csv'), DfParser('norm_test.csv')\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f67b8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_worker(\n",
    "    gpu,\n",
    "    ngpus_per_node,\n",
    "    config, #batchsize, maxpadding, baselr, warmup, num_epochs, accum_iter, file prefix\n",
    "    is_distributed=False,\n",
    "):\n",
    "    print(f\"Train worker process using GPU: {gpu} for training\", flush=True)\n",
    "    torch.cuda.set_device(gpu)\n",
    "\n",
    "    d_model = 512\n",
    "    model = make_model(loc_num=189, var_num=3, past=14, pred=3, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1)\n",
    "    model.cuda(gpu)\n",
    "    module = model\n",
    "    #print(next(model.parameters()).is_cuda)\n",
    "    is_main_process = True\n",
    "    \n",
    "    if is_distributed:\n",
    "        dist.init_process_group(\n",
    "            \"nccl\", init_method=\"env://\", rank=gpu, world_size=ngpus_per_node\n",
    "        )\n",
    "        model = DDP(model, device_ids=[gpu])\n",
    "        module = model.module\n",
    "        is_main_process = gpu == 0\n",
    "\n",
    "    train_dataloader, valid_dataloader, test_dataloader = create_dataloaders(\n",
    "        gpu,\n",
    "        batch_size=config[\"batch_size\"] // ngpus_per_node,\n",
    "        is_distributed=is_distributed,\n",
    "    )\n",
    "    del test_dataloader\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, d_model, factor=1, warmup=config[\"warmup\"]\n",
    "        ),\n",
    "    )\n",
    "    train_state = TrainState()\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(f'{config[\"file_prefix\"]}latest.pt')\n",
    "        print(f'Model checkpoint found')\n",
    "        try:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            del checkpoint #this was important when running for me as I ran out of memory if I loaded\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            print('Error Loading')\n",
    "    except:\n",
    "        print(f'Model Checkpoint not found.')\n",
    "    \n",
    "    \n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        if is_distributed:\n",
    "            train_dataloader.sampler.set_epoch(epoch)\n",
    "            valid_dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        model.train()\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n",
    "        \"Recall our batch class takes in batch(src,tgt)\"\n",
    "        _, train_state = run_epoch(\n",
    "            (ProcessRaw(b) for b in train_dataloader), \n",
    "            model,\n",
    "            nn.MSELoss(),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train+log\",\n",
    "            accum_iter=config[\"accum_iter\"],\n",
    "            train_state=train_state,\n",
    "        )\n",
    "\n",
    "        GPUtil.showUtilization()\n",
    "        if is_main_process:\n",
    "            file_path = f'{config[\"file_prefix\"]}latest.pt'\n",
    "            torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, file_path)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            sloss = run_epoch(\n",
    "                (ProcessRaw(b) for b in valid_dataloader),\n",
    "                model,\n",
    "                nn.MSELoss(),\n",
    "                DummyOptimizer(),\n",
    "                DummyScheduler(),\n",
    "                mode=\"eval\",\n",
    "            )\n",
    "            print(sloss)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if is_main_process:\n",
    "        file_path = \"%sfinal.pt\" % config[\"file_prefix\"]\n",
    "        torch.save(module.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b1bc1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ====\n",
      "(tensor(7.8195e-07, device='cuda:0'), <__main__.TrainState object at 0x0000011FE85B5B88>)\n",
      "Test ====\n",
      "(tensor(7.8053e-07, device='cuda:0'), <__main__.TrainState object at 0x0000011FE85B5B88>)\n"
     ]
    }
   ],
   "source": [
    "def train_distributed_model(config):\n",
    "    from the_annotated_transformer import train_worker\n",
    "\n",
    "    ngpus = torch.cuda.device_count()\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12356\"\n",
    "    print(f\"Number of GPUs detected: {ngpus}\")\n",
    "    print(\"Spawning training processes ...\")\n",
    "    mp.spawn(\n",
    "        train_worker,\n",
    "        nprocs=ngpus,\n",
    "        args=(ngpus, config, True),\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "config = {\n",
    "        \"batch_size\": 16,\n",
    "        \"distributed\": False,\n",
    "        \"num_epochs\": 8,\n",
    "        \"accum_iter\": 10,\n",
    "        \"base_lr\": 1.0,\n",
    "        \"max_padding\": 72,\n",
    "        \"warmup\": 3000,\n",
    "        \"file_prefix\": \"norm_covid_model_\",\n",
    "    }\n",
    "\n",
    "    \n",
    "def train_model(config):\n",
    "    if config[\"distributed\"]:\n",
    "        train_distributed_model(\n",
    "            config\n",
    "        )\n",
    "    else:\n",
    "        train_worker(\n",
    "            0, 1, config, False\n",
    "        )\n",
    "\n",
    "\n",
    "def load_trained_model(config): #loads a trained model or trains it if there is none\n",
    "    model_path = \"norm_covid_model_final.pt\"\n",
    "    if not exists(model_path):\n",
    "        train_model(config)\n",
    "    model = make_model(loc_num=189, var_num=3, past=14, pred=3, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1)\n",
    "    model.load_state_dict(torch.load(\"norm_covid_model_final.pt\"))\n",
    "    return model\n",
    "\n",
    "def eval_model(gpu, path):\n",
    "    torch.cuda.set_device(gpu)\n",
    "    model = make_model(loc_num=189, var_num=3, past=14, pred=3, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1)\n",
    "    model.cuda(gpu)\n",
    "    model_path=path \n",
    "    loaded_model = torch.load(path)\n",
    "    model.load_state_dict(loaded_model['model_state_dict'])\n",
    "    del loaded_model\n",
    "    \n",
    "    train_dataloader, valid_dataloader, test_dataloader = create_dataloaders(\n",
    "        gpu,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        is_distributed=False,\n",
    "    )\n",
    "    del train_dataloader\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Validation ====\", flush=True)\n",
    "    with torch.no_grad():\n",
    "            model.eval()\n",
    "            sloss = run_epoch(\n",
    "                (ProcessRaw(b) for b in valid_dataloader),\n",
    "                model,\n",
    "                nn.MSELoss(),\n",
    "                DummyOptimizer(),\n",
    "                DummyScheduler(),\n",
    "                mode=\"eval\",\n",
    "            )\n",
    "            print(sloss)\n",
    "            \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Test ====\", flush=True)\n",
    "    with torch.no_grad():\n",
    "            model.eval()\n",
    "            tloss = run_epoch(\n",
    "                (ProcessRaw(b) for b in test_dataloader),\n",
    "                model,\n",
    "                nn.MSELoss(),\n",
    "                DummyOptimizer(),\n",
    "                DummyScheduler(),\n",
    "                mode=\"eval\",\n",
    "            )\n",
    "            print(tloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Run the below functions\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#train_model(config)\n",
    "eval_model(0, 'norm_covid_model_latest.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e3ae558",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 8.9785e-02, 8.9970e-02, 9.0113e-02, 9.0196e-02, 9.0449e-02,\n",
      "         9.0587e-02, 9.0744e-02, 9.0901e-02, 9.1111e-02, 9.1297e-02, 9.1404e-02,\n",
      "         9.1823e-02, 9.2005e-02, 9.2193e-02, 2.1644e+00, 2.1651e+00, 2.1658e+00,\n",
      "         2.1666e+00, 2.1673e+00, 2.1680e+00, 2.1687e+00, 2.1694e+00, 2.1700e+00,\n",
      "         2.1707e+00, 2.1713e+00, 2.1721e+00, 2.1729e+00, 2.1737e+00, 9.7885e-07,\n",
      "         1.9842e-06, 5.8202e-07, 7.6721e-07, 8.2012e-07, 6.6139e-07, 8.9949e-07,\n",
      "         1.1905e-06, 1.2434e-06, 7.4075e-07, 3.4392e-07, 1.9312e-06, 8.9949e-07,\n",
      "         5.2911e-07, 1.1905e-06, 1.3492e-06, 8.4657e-07],\n",
      "        [0.0000e+00, 1.0806e-01, 1.0806e-01, 1.0806e-01, 1.0873e-01, 1.0873e-01,\n",
      "         1.0873e-01, 1.0873e-01, 1.0873e-01, 1.0873e-01, 1.0873e-01, 1.0941e-01,\n",
      "         1.0941e-01, 1.0941e-01, 1.0941e-01, 2.3221e+00, 2.3229e+00, 2.3237e+00,\n",
      "         2.3245e+00, 2.3252e+00, 2.3260e+00, 2.3267e+00, 2.3276e+00, 2.3287e+00,\n",
      "         2.3300e+00, 2.3312e+00, 2.3325e+00, 2.3338e+00, 2.3351e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 7.0371e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 7.7250e-06, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0018e-01, 1.0040e-01, 1.0047e-01, 1.0061e-01, 1.0080e-01,\n",
      "         1.0088e-01, 1.0096e-01, 1.0104e-01, 1.0121e-01, 1.0127e-01, 1.0137e-01,\n",
      "         1.0152e-01, 1.0159e-01, 1.0165e-01, 2.2311e+00, 2.2326e+00, 2.2341e+00,\n",
      "         2.2356e+00, 2.2369e+00, 2.2381e+00, 2.2393e+00, 2.2405e+00, 2.2416e+00,\n",
      "         2.2429e+00, 2.2441e+00, 2.2455e+00, 2.2470e+00, 2.2485e+00, 1.0318e-06,\n",
      "         5.0265e-06, 3.1747e-07, 6.0848e-07, 3.4392e-06, 1.5609e-06, 1.0318e-06,\n",
      "         1.2699e-06, 5.6879e-06, 2.6455e-07, 8.2012e-07, 2.3016e-06, 8.2012e-07,\n",
      "         9.7885e-07, 9.5240e-07, 3.3598e-06, 1.8519e-07],\n",
      "        [0.0000e+00, 3.1427e-02, 3.1614e-02, 3.1819e-02, 3.2040e-02, 3.2260e-02,\n",
      "         3.2484e-02, 3.2691e-02, 3.2891e-02, 3.3068e-02, 3.3263e-02, 3.3475e-02,\n",
      "         3.3682e-02, 3.3878e-02, 3.4071e-02, 3.2177e-01, 3.2927e-01, 3.3647e-01,\n",
      "         3.4346e-01, 3.5023e-01, 3.5686e-01, 3.6339e-01, 3.6999e-01, 3.7680e-01,\n",
      "         3.8405e-01, 3.9166e-01, 3.9965e-01, 4.0806e-01, 4.1672e-01, 1.0053e-06,\n",
      "         1.1905e-06, 1.2963e-06, 1.5080e-06, 1.3228e-06, 1.0582e-06, 9.7885e-07,\n",
      "         1.0053e-06, 1.4286e-06, 1.4815e-06, 9.7885e-07, 1.0847e-06, 1.3228e-06,\n",
      "         1.6402e-06, 1.0582e-06, 8.2012e-07, 1.3228e-06],\n",
      "        [0.0000e+00, 3.7820e-02, 3.7844e-02, 3.7864e-02, 3.7893e-02, 3.7912e-02,\n",
      "         3.7931e-02, 3.7957e-02, 3.7995e-02, 3.8035e-02, 3.8071e-02, 3.8119e-02,\n",
      "         3.8151e-02, 3.8187e-02, 3.8236e-02, 1.2903e+00, 1.2971e+00, 1.3028e+00,\n",
      "         1.3085e+00, 1.3138e+00, 1.3187e+00, 1.3234e+00, 1.3278e+00, 1.3321e+00,\n",
      "         1.3368e+00, 1.3409e+00, 1.3448e+00, 1.3487e+00, 1.3524e+00, 1.3228e-07,\n",
      "         3.4392e-07, 1.5873e-07, 5.2911e-08, 2.6455e-08, 1.5873e-07, 2.9101e-07,\n",
      "         4.7620e-07, 5.5556e-07, 2.1164e-07, 1.5873e-07, 2.1164e-07, 1.5873e-07,\n",
      "         1.3228e-07, 1.0582e-07, 1.3228e-07, 7.9366e-08],\n",
      "        [0.0000e+00, 2.9261e-02, 2.9496e-02, 2.9722e-02, 2.9941e-02, 3.0134e-02,\n",
      "         3.0357e-02, 3.0578e-02, 3.0809e-02, 3.1029e-02, 3.1236e-02, 3.1427e-02,\n",
      "         3.1614e-02, 3.1819e-02, 3.2040e-02, 2.4600e-01, 2.5327e-01, 2.6042e-01,\n",
      "         2.6765e-01, 2.7508e-01, 2.8268e-01, 2.9041e-01, 2.9825e-01, 3.0610e-01,\n",
      "         3.1403e-01, 3.2177e-01, 3.2927e-01, 3.3647e-01, 3.4346e-01, 1.0847e-06,\n",
      "         1.3228e-06, 8.2012e-07, 1.1905e-06, 1.1640e-06, 1.4286e-06, 1.4286e-06,\n",
      "         1.4550e-06, 1.5080e-06, 1.1376e-06, 1.0053e-06, 1.1905e-06, 1.2963e-06,\n",
      "         1.5080e-06, 1.3228e-06, 1.0582e-06, 9.7885e-07],\n",
      "        [0.0000e+00, 2.4230e-02, 2.4319e-02, 2.4414e-02, 2.4526e-02, 2.4632e-02,\n",
      "         2.4734e-02, 2.4834e-02, 2.4929e-02, 2.5036e-02, 2.5174e-02, 2.5309e-02,\n",
      "         2.5451e-02, 2.5586e-02, 2.5707e-02, 8.5368e-02, 8.8813e-02, 9.2361e-02,\n",
      "         9.5901e-02, 9.9522e-02, 1.0316e-01, 1.0692e-01, 1.1081e-01, 1.1490e-01,\n",
      "         1.1912e-01, 1.2357e-01, 1.2818e-01, 1.3297e-01, 1.3795e-01, 5.8202e-07,\n",
      "         9.5240e-07, 8.7303e-07, 6.6139e-07, 6.8784e-07, 8.7303e-07, 1.0582e-06,\n",
      "         4.7620e-07, 6.3493e-07, 8.2012e-07, 9.2594e-07, 7.6721e-07, 7.1430e-07,\n",
      "         3.9683e-07, 6.8784e-07, 8.4657e-07, 1.1111e-06],\n",
      "        [0.0000e+00, 1.0298e-01, 1.0298e-01, 1.0298e-01, 1.0298e-01, 1.0319e-01,\n",
      "         1.0319e-01, 1.0319e-01, 1.0319e-01, 1.0319e-01, 1.0319e-01, 1.0319e-01,\n",
      "         1.0359e-01, 1.0359e-01, 1.0359e-01, 2.2708e+00, 2.2715e+00, 2.2722e+00,\n",
      "         2.2729e+00, 2.2736e+00, 2.2743e+00, 2.2750e+00, 2.2757e+00, 2.2764e+00,\n",
      "         2.2770e+00, 2.2778e+00, 2.2784e+00, 2.2791e+00, 2.2798e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5821e-06, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.6244e-06, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 4.6174e-02, 4.6244e-02, 4.6305e-02, 4.6362e-02, 4.6413e-02,\n",
      "         4.6477e-02, 4.6550e-02, 4.6626e-02, 4.6703e-02, 4.6768e-02, 4.6829e-02,\n",
      "         4.6888e-02, 4.6957e-02, 4.7033e-02, 1.5734e+00, 1.5751e+00, 1.5768e+00,\n",
      "         1.5784e+00, 1.5801e+00, 1.5817e+00, 1.5834e+00, 1.5850e+00, 1.5866e+00,\n",
      "         1.5881e+00, 1.5895e+00, 1.5909e+00, 1.5923e+00, 1.5940e+00, 1.0847e-06,\n",
      "         3.9683e-07, 4.2329e-07, 1.1640e-06, 5.0265e-07, 7.4075e-07, 7.9366e-07,\n",
      "         5.0265e-07, 2.9101e-07, 2.9101e-07, 6.0848e-07, 7.6721e-07, 6.6139e-07,\n",
      "         5.2911e-07, 4.7620e-07, 4.2329e-07, 2.3810e-07],\n",
      "        [0.0000e+00, 8.0201e-02, 8.0560e-02, 8.0843e-02, 8.1136e-02, 8.1536e-02,\n",
      "         8.1912e-02, 8.2285e-02, 8.2590e-02, 8.2862e-02, 8.3071e-02, 8.3305e-02,\n",
      "         8.3600e-02, 8.3881e-02, 8.4151e-02, 2.0468e+00, 2.0528e+00, 2.0586e+00,\n",
      "         2.0642e+00, 2.0693e+00, 2.0743e+00, 2.0791e+00, 2.0834e+00, 2.0874e+00,\n",
      "         2.0912e+00, 2.0948e+00, 2.0982e+00, 2.1015e+00, 2.1048e+00, 3.1482e-06,\n",
      "         1.8519e-06, 3.8625e-06, 4.0477e-06, 4.6562e-06, 4.3916e-06, 3.9683e-06,\n",
      "         2.5662e-06, 2.1693e-06, 3.3069e-06, 3.1482e-06, 3.8360e-06, 2.9630e-06,\n",
      "         3.6244e-06, 2.7778e-06, 7.9366e-07, 2.8836e-06]])\n",
      "tensor([[0.0000e+00, 2.3810e-07, 2.3810e-07, 2.6455e-07, 2.6455e-07, 2.9101e-07,\n",
      "         3.1747e-07, 3.4392e-07, 3.9683e-07, 3.9683e-07, 6.3493e-07, 8.7303e-07,\n",
      "         8.7303e-07, 1.1905e-06, 1.1905e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6455e-08],\n",
      "        [0.0000e+00, 3.6357e-02, 3.6429e-02, 3.6494e-02, 3.6550e-02, 3.6593e-02,\n",
      "         3.6648e-02, 3.6705e-02, 3.6760e-02, 3.6810e-02, 3.6850e-02, 3.6883e-02,\n",
      "         3.6916e-02, 3.6953e-02, 3.6992e-02, 5.9774e-01, 6.0716e-01, 6.1681e-01,\n",
      "         6.2655e-01, 6.3636e-01, 6.4618e-01, 6.5612e-01, 6.6608e-01, 6.7611e-01,\n",
      "         6.8629e-01, 6.9668e-01, 7.0730e-01, 7.1809e-01, 7.2903e-01, 7.9366e-07,\n",
      "         1.1905e-06, 9.2594e-07, 7.1430e-07, 5.0265e-07, 1.2170e-06, 8.2012e-07,\n",
      "         8.4657e-07, 1.1111e-06, 3.1747e-07, 7.1430e-07, 7.6721e-07, 1.3757e-06,\n",
      "         7.9366e-07, 3.4392e-07, 6.8784e-07, 4.4974e-07],\n",
      "        [0.0000e+00, 2.7879e-02, 2.8108e-02, 2.8334e-02, 2.8534e-02, 2.8761e-02,\n",
      "         2.9014e-02, 2.9261e-02, 2.9496e-02, 2.9722e-02, 2.9941e-02, 3.0134e-02,\n",
      "         3.0357e-02, 3.0578e-02, 3.0809e-02, 2.0199e-01, 2.0894e-01, 2.1613e-01,\n",
      "         2.2350e-01, 2.3095e-01, 2.3844e-01, 2.4600e-01, 2.5327e-01, 2.6042e-01,\n",
      "         2.6765e-01, 2.7508e-01, 2.8268e-01, 2.9041e-01, 2.9825e-01, 1.1905e-06,\n",
      "         8.2012e-07, 7.1430e-07, 9.2594e-07, 1.4021e-06, 1.4021e-06, 1.0847e-06,\n",
      "         1.3228e-06, 8.2012e-07, 1.1905e-06, 1.1640e-06, 1.4286e-06, 1.4286e-06,\n",
      "         1.4550e-06, 1.5080e-06, 1.1376e-06, 1.0053e-06],\n",
      "        [0.0000e+00, 3.7124e-02, 3.7146e-02, 3.7174e-02, 3.7203e-02, 3.7230e-02,\n",
      "         3.7255e-02, 3.7276e-02, 3.7291e-02, 3.7307e-02, 3.7327e-02, 3.7345e-02,\n",
      "         3.7363e-02, 3.7381e-02, 3.7394e-02, 7.7500e-01, 7.8699e-01, 7.9911e-01,\n",
      "         8.1123e-01, 8.2325e-01, 8.3512e-01, 8.4707e-01, 8.5922e-01, 8.7169e-01,\n",
      "         8.8411e-01, 8.9661e-01, 9.0915e-01, 9.2282e-01, 9.3692e-01, 2.9101e-07,\n",
      "         7.6721e-07, 7.4075e-07, 3.4392e-07, 2.9101e-07, 8.2012e-07, 5.5556e-07,\n",
      "         2.1164e-07, 1.8783e-06, 5.5556e-07, 4.4974e-07, 1.8519e-07, 4.4974e-07,\n",
      "         3.9683e-07, 2.9101e-07, 7.9366e-07, 5.5556e-07],\n",
      "        [0.0000e+00, 8.5765e-02, 8.5892e-02, 8.6017e-02, 8.6164e-02, 8.6333e-02,\n",
      "         8.6516e-02, 8.6694e-02, 8.6859e-02, 8.6986e-02, 8.7142e-02, 8.7246e-02,\n",
      "         8.7428e-02, 8.7587e-02, 8.7766e-02, 2.1248e+00, 2.1268e+00, 2.1288e+00,\n",
      "         2.1306e+00, 2.1324e+00, 2.1342e+00, 2.1358e+00, 2.1387e+00, 2.1412e+00,\n",
      "         2.1436e+00, 2.1459e+00, 2.1482e+00, 2.1504e+00, 2.1526e+00, 1.3228e-06,\n",
      "         7.6721e-07, 5.0265e-07, 2.5662e-06, 2.3810e-06, 2.2223e-06, 2.5397e-06,\n",
      "         2.4074e-06, 3.7038e-07, 1.5344e-06, 1.1111e-06, 2.3810e-06, 1.8519e-06,\n",
      "         1.8519e-06, 1.4286e-06, 7.6721e-07, 1.4021e-06],\n",
      "        [0.0000e+00, 6.3146e-03, 6.4012e-03, 6.4799e-03, 6.5667e-03, 6.6705e-03,\n",
      "         6.7675e-03, 6.8799e-03, 7.0014e-03, 7.1095e-03, 7.2233e-03, 7.3439e-03,\n",
      "         7.4757e-03, 7.6011e-03, 7.7405e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1111e-06,\n",
      "         6.8784e-07, 1.8783e-06, 1.5080e-06, 1.1905e-06, 1.4286e-06, 1.4286e-06,\n",
      "         8.2012e-07, 1.1111e-06, 1.7990e-06, 1.4021e-06, 2.1958e-06, 1.5873e-06,\n",
      "         1.6402e-06, 1.6402e-06, 1.9312e-06, 1.5080e-06],\n",
      "        [0.0000e+00, 9.3027e-02, 9.3565e-02, 9.3771e-02, 9.3990e-02, 9.4209e-02,\n",
      "         9.4521e-02, 9.4777e-02, 9.5091e-02, 9.5448e-02, 9.5649e-02, 9.5861e-02,\n",
      "         9.5980e-02, 9.6282e-02, 9.6350e-02, 2.1771e+00, 2.1780e+00, 2.1789e+00,\n",
      "         2.1800e+00, 2.1812e+00, 2.1824e+00, 2.1838e+00, 2.1854e+00, 2.1871e+00,\n",
      "         2.1889e+00, 2.1906e+00, 2.1923e+00, 2.1939e+00, 2.1955e+00, 2.3810e-07,\n",
      "         2.1164e-06, 1.2699e-06, 1.1640e-06, 1.1111e-06, 2.1429e-06, 1.0847e-06,\n",
      "         7.4075e-07, 2.0900e-06, 8.7303e-07, 1.2963e-06, 3.4392e-07, 2.5926e-06,\n",
      "         2.6455e-08, 2.5926e-06, 1.8783e-06, 1.6931e-06],\n",
      "        [0.0000e+00, 2.3724e-02, 2.3803e-02, 2.3895e-02, 2.3989e-02, 2.4080e-02,\n",
      "         2.4155e-02, 2.4230e-02, 2.4319e-02, 2.4414e-02, 2.4526e-02, 2.4632e-02,\n",
      "         2.4734e-02, 2.4834e-02, 2.4929e-02, 6.8954e-02, 7.1269e-02, 7.3720e-02,\n",
      "         7.6314e-02, 7.9102e-02, 8.2124e-02, 8.5368e-02, 8.8813e-02, 9.2361e-02,\n",
      "         9.5901e-02, 9.9522e-02, 1.0316e-01, 1.0692e-01, 1.1081e-01, 7.6721e-07,\n",
      "         8.9949e-07, 8.4657e-07, 8.7303e-07, 8.4657e-07, 6.3493e-07, 5.8202e-07,\n",
      "         9.5240e-07, 8.7303e-07, 6.6139e-07, 6.8784e-07, 8.7303e-07, 1.0582e-06,\n",
      "         4.7620e-07, 6.3493e-07, 8.2012e-07, 9.2594e-07],\n",
      "        [0.0000e+00, 3.7291e-02, 3.7307e-02, 3.7327e-02, 3.7345e-02, 3.7363e-02,\n",
      "         3.7381e-02, 3.7394e-02, 3.7412e-02, 3.7427e-02, 3.7442e-02, 3.7458e-02,\n",
      "         3.7469e-02, 3.7480e-02, 3.7494e-02, 8.5922e-01, 8.7169e-01, 8.8411e-01,\n",
      "         8.9661e-01, 9.0915e-01, 9.2282e-01, 9.3692e-01, 9.5136e-01, 9.6531e-01,\n",
      "         9.7943e-01, 9.9323e-01, 1.0069e+00, 1.0202e+00, 1.0330e+00, 2.1164e-07,\n",
      "         1.8783e-06, 5.5556e-07, 4.4974e-07, 1.8519e-07, 4.4974e-07, 3.9683e-07,\n",
      "         2.9101e-07, 7.9366e-07, 5.5556e-07, 6.0848e-07, 5.5556e-07, 2.3810e-07,\n",
      "         3.1747e-07, 1.8519e-07, 3.4392e-07, 1.5873e-07],\n",
      "        [0.0000e+00, 4.7148e-02, 4.7228e-02, 4.7297e-02, 4.7359e-02, 4.7421e-02,\n",
      "         4.7504e-02, 4.7590e-02, 4.7682e-02, 4.7778e-02, 4.7868e-02, 4.7944e-02,\n",
      "         4.8022e-02, 4.8116e-02, 4.8228e-02, 1.5958e+00, 1.5977e+00, 1.6000e+00,\n",
      "         1.6025e+00, 1.6052e+00, 1.6109e+00, 1.6169e+00, 1.6229e+00, 1.6291e+00,\n",
      "         1.6356e+00, 1.6424e+00, 1.6493e+00, 1.6538e+00, 1.6583e+00, 4.7620e-07,\n",
      "         4.2329e-07, 2.3810e-07, 7.1430e-07, 4.4974e-07, 6.8784e-07, 6.0848e-07,\n",
      "         5.2911e-07, 2.6455e-07, 1.3228e-07, 7.1430e-07, 7.1430e-07, 6.3493e-07,\n",
      "         6.6139e-07, 6.3493e-07, 3.7038e-07, 3.1747e-07]])\n"
     ]
    }
   ],
   "source": [
    "#To check dataloaders are working as intended\n",
    "'''\n",
    "train, valid, test = create_dataloaders(0)\n",
    "for b in test:\n",
    "    print(b[:10])\n",
    "    break\n",
    "\n",
    "for b in valid:\n",
    "    print(b[:10])\n",
    "    break\n",
    "    \n",
    "for b in test:\n",
    "    print(b[:10])\n",
    "    break\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ed62717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
